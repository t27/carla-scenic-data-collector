# Baseline Modelling

This section includes 2 baseline models that help analyze the scenario data. There are 2 types of models that we use, one is a frame classifier and the other is an Autoencoder model that runs on a DTW representation of the scenarios.

## Preliminary Dataprep

Here we will generate the frame and agent level data from the consolidated scenario data that was generated by the `record_data.sh` script. This frame and agent data will be generated in a folder called `agent_maps/` in the root directory. 

To run the preliminary dataprep process, follow the following steps.

1. Run the `record_data.sh` script. If you want to customize what data is being stored from the simulation refer to the [Customizing Recordings](../custom_recordings.md) document.
2. Update the `folders` list in the `generate_agent_maps.py` file in the root directory. Ensure to only list the directories that were generated after you run the `record_data.sh` script. If you customized what data was recorded in `recorder.py`, ensure that you update `generate_agent_maps.py` to ensure that data is being included in the agent maps as well. (Note, check the `CarlaCsvParser` class, in particular its `run` function)
3. Step 2 will now have generated the individual agent maps, now we are ready to continue to the model specific steps. The model specific steps and the `generate_agent_maps.py` share some convention in the filenames, this helps in consistently and efficiently parsing the agent data. Refer to [Agent Map Filename Convention](agent_map_filename_convention.md) for the details(though by default everything should work automatically).

## Frame Classifier

This is a Random Forest based classifier model that runs on a set of precalculated features. Each scenario includes data for each "frame". A frame is the the data for a given timestamp. Each frame contains multiple agents and their positions, velocities etc.

For extracting features for the Random Forest model, for each frame we identify the minimum and maximum values of acceleration, velocity, angular velocity in each of the 3 directions(x, y, z in the global frame). We use these as the features for our simple classification models. The features are - `"max_velocity_x", "max_velocity_y", "max_velocity_z", "max_ang_velocity_x", "max_ang_velocity_y", "max_ang_velocity_z", "min_velocity_x", "min_velocity_y", "min_velocity_z", "min_ang_velocity_x", "min_ang_velocity_y", "min_ang_velocity_z", "max_acc_x", "max_acc_y", "max_acc_z", "min_acc_x", "min_acc_y", "min_acc_z"`.

To run this classifier, run the following commands from the *root folder*

1. Ensure the Preliminary Dataprep steps are completed.
2. `python modelling/frame_classifier/run.py`
3. This will train a random forest classifier and also print the results. It will also show the permutation importance plots for the model. Refer to the individual function calls inside `frame_classifier/run.py` for an example on how to use the generated model. You may write your own "run" file to either save the model or even infer on another dataset.


## DTW Autoencoder

The DTW Autoencoder converts each "scenario" into a DTW map. The DTW map is always wrt a given "ego" vehicle. We can consider any vehicle in the scenario as an "ego" vehicle. In our case, for each scenario, we consider ALL vehicles as ego one by one. So for a scenario with 5 vehicles, we will have 5 different sets of data points (one for with each vehicle to have the ego perspective). 

One DTW map includes a tensor of upto 10 interactions. An interaction is generated using the position information of the ego vehicle and another vehicle in the scenario. The trajectories of the 2 vehicles generates one DTW frame(we use the DTW cost map in the 2D form as one frame). 10 DTW frames make up one tensor, which we refer to as a DTW map. For one DTW map, we will have the current ego vehicle in all the interactions.

So in a scenario where we have 11 vehicles, the number of possible "ego" vehicles is 11. And for each "ego" vehicle, we have 10 "other" vehicles(all vehicles besides itself). Hence for each ego vehicle we will have 10 DTW frames. These 10 frames are stacked to produce a tensor which we refer to as the DTW map.

To train the Autoencoder, we run the following steps,

1. Ensure the Preliminary Dataprep steps are completed
2. `python modelling/dtw_autoencoder/train.py` - This will train the DTW model and save the model checkpoints.
3. Use the autoencoder's "encoder" to return an embedding for a given scenario (using the `ScenarioModel`'s `embedding()` function.) This embedding can further be used for clustering or even training a supervised model